# -----------------------------------------------------------------------------
# Author    : Albert Akhriev, albert_akhriev@ie.ibm.com
# Copyright : IBM Research Ireland, 2017-2018
# -----------------------------------------------------------------------------

""" This script is to be used as a testing framework for the AllScale project.
    The script takes 5 configurations of varying size and computes the parallel
    scalability and throughput under different runtime configurations, namely:
    ALLSCALE_MONITOR=0/1
    ALLSCALE_RESILIENCE=0/1

    1) Weak scaling is implemented as follows
    The first two implementations (tiny and small) are of fixed size ([4,2], [8,4],
    respectively) and run on 1 and 4 threads resepectively.
    These are also used to assess correctness of solution. The computed solution
    (using forward model computed sensor data) is compared to a benchmark "correct
    solution" stored in allscale_amdados/correct_solution.
    The remaining three tests are function of number of available threads. Largest
    problem size uses all available threads (Nt) and assigns 8 subdomains per thread.
    Medium and large experiments use same ratio of subdomains to threads in form:
    np.arange(8,Nt, (Nt-8)/3).

    2) Strong scaling: same problem size to be processed by a varying number of threads
    Number of domains assigned as 4 X max_number_threads.
    e.g. for ARM cluster with max 256 threads, a problem size of 1024 subdomains is
    assigned and the number of threads increased from 1 to max.

    It saves the execution time of each
    simulation in a file that can be used to plot the scalability profile.
    Essential parameters, listed in the first lines, include the set of
    problem sizes and integration period.
      Each simulation is twofold. First, we run the Python forward solver that
    generates the ground-truth and observations ("ObservationsGenerator.py").
    The Python code itself uses the C++ code running in the special mode for
    generating sensor locations (scenario "sensors"). Second, the C++ data
    assimilation application is launched (scenario "simulation") with
    observations generated by "ObservationsGenerator.py".
      The results of all the simulations are accumulated in the output
    directory and can be visualized later on by the script "Visualize.py".
      The configuration file "amdados.conf" is used in all the simulations with
    modification of three parameters: grid sizes (number of subdomains) in both
    dimensions and integration time. Other parameters remain intact. It is not
    recommended to tweak parameters unless their meaning is absolutely clear.
      If you had modified the parameters, please, consider to rerun this script
    because the results in the output directory a not valid any longer.
      The script was designed to fulfil the formal requirements of the
    Allscale project.
"""
print(__doc__)

from timeit import default_timer as timer
import os, cmd
from RandObservationsGenerator import Amdados2D_quick
from ObservationsGenerator import InitDependentParams, Amdados2D

from Utility import *

Nt = 88   # MaxThreads
thread_seq = np.arange(8,Nt, (Nt-8)/3)

ExperimentConfigs = [ [1, [4,2] ],                                          # Tiny
                      [4, [8, 2]],                                          # Small
                      [int(thread_seq[1]), [int(thread_seq[1]), 8]],        # Medium
                      [int(thread_seq[2]), [int(thread_seq[2]), 8]],        # Large
                      [Nt, [Nt, 8]]]                                        # Real-world


execute_time =  np.zeros([len(ExperimentConfigs) * 4, 6])
# Integration period in seconds.
IntegrationPeriod = 25 
IntegrationNsteps = 50
# Path to the C++ executable.
AMDADOS_EXE = "build/app/amdados"
ResultsFileName = "AMDADOS_weakScaling.txt"

def VerifyCorrectSolution(Grid):
    nx = 16*Grid[0]
    ny = 16*Grid[1]
    filename = "field_Nx" + str(nx) + "_Ny" + str(ny) + "_Nt" + str(IntegrationNsteps) + ".bin"
    simulation = np.loadtxt("./output/" + filename)
    reference = np.loadtxt("./correctsolution/" + filename)
    assert len(simulation) == len(reference)
    sim_array = np.zeros([nx, ny])
    ref_array = np.zeros([nx, ny])
    for i in range(0,len(simulation)):
        sim_array[int(simulation[i,1]),int(simulation[i,2]) ] = simulation[i,3]
        ref_array[int(reference[i,1]),int(reference[i,2]) ] = reference[i,3]
        diff = sum(sum(sim_array - ref_array)) #  "computed results do not match with correct solution"
    return diff


if __name__ == "__main__":
    try:
        # Read configuration file.
        conf = Configuration("amdados.conf")
        # Create the output directory, if it does not exist.
        if not os.path.isdir(conf.output_dir):
            os.mkdir(conf.output_dir)
        # Check existence of "amdados" application executable.
        assert os.path.isfile(AMDADOS_EXE), "amdados executable was not found"
        HeaderTxt = ["ProblemSize, NThreads, ALLSCALE_MONITOR, ALLSCALE_RESILIENCE, TotalRuntime, Throughput(Sdom/s)"]
        print('filename = ',os.path.join(conf.output_dir, ResultsFileName))
        np.savetxt(os.path.join(conf.output_dir, ResultsFileName), HeaderTxt, fmt = '%s')
        i = 0
        # For all the grid sizes in the list ...
        for domain in range(0, len(ExperimentConfigs)):
            grid = ExperimentConfigs[domain][1]
            Nproc = ExperimentConfigs[domain][0]
            # Modify parameters given the current grid size.
            setattr(conf, "num_subdomains_x", int(grid[0]))
            setattr(conf, "num_subdomains_y", int(grid[1]))
            setattr(conf, "integration_period", int(IntegrationPeriod))
            setattr(conf, "integration_nsteps", int(IntegrationNsteps))
            InitDependentParams(conf)
            conf.PrintParameters()
            config_file = conf.WriteParameterFile("scalability_test.conf")
            os.sync()
            # Python simulator generates the ground-truth and observations.
            # We explore two different configurations here; tiny and small uses
            # forward model generator and checks correctness of solution
            # Others use a rapid observation generator

            if (domain < 2): # Tiny and small configurations
                Amdados2D(config_file, False)    # observation generator using Python Forward Model
            else:
                Amdados2D_quick(config_file, False) # observation generator using Rapid observation generator

            # For each test size, we need to run different configurations investigating
            # AllScale Resilience and Monitoring (i.e. =0/1)
            #  Run C++ data assimilation application.
            for MonitorFlag in [0,1]:
                for ResilienceFlag in [0,1]:
                    print("##################################################")
                    print("Testing Framework for AllScale project")
                    print("Simulation by 'amdados' to check scalability and correctness")
                    print("Testing Configuration Setup")
                    print("GridSize =", grid, "ALLSCALE_MONITOR = ", MonitorFlag,
                    "ALLSCALE_RESILIENCE = ", ResilienceFlag)
                    print("##################################################")
                    print(AMDADOS_EXE, config_file)
                    output = subprocess.Popen([AMDADOS_EXE, "--scenario", "simulation",
                        "--config", config_file, "--hpx:threads=" + str(Nproc), "--hpx:bind=none"], stdout=subprocess.PIPE,
                                      env=dict(os.environ, ALLSCALE_MONITOR=str(MonitorFlag), ALLSCALE_RESILIENCE=str(ResilienceFlag)))
                    output.wait()
                    assert output.returncode == 0, "amdados returned non-zero status"
                    if (domain < 2): # Tiny and small configurations
                        diff = VerifyCorrectSolution(grid) # assert correct solution for tiny and small configurations
                        print('diff = ', diff)
                        assert diff == 0., "solution doesn't match with correct benchmark solution"
                    # Strip the execution time from stdout, both total simulation time
                    # and throughput (subdomain/s)
                    strip_output = str(output.communicate()[0]).split('\\n')
                    for line in strip_output:
                        if re.search("Simulation", line):
                            simtime_string = line
                        if re.search("Throughput", line):
                            throughput_string = line
                    simtime_secs = float(simtime_string.split(' ')[2][:-1])
                    throughput_secs = float(throughput_string.split(' ')[1])

                    # Get the execution time and corresponding (global) problem size
                    # and save the current scalability profile into the file.
                    problem_size = ( conf.num_subdomains_x * conf.num_subdomains_y )
                    execute_time[i, :] = [problem_size, Nproc, MonitorFlag, ResilienceFlag, simtime_secs, throughput_secs]
                    i += 1
                    np.savetxt(os.path.join(conf.output_dir, ResultsFileName),
                            execute_time)

    except subprocess.CalledProcessError as error:
        traceback.print_exc()
        if error.output is not None:
            print("ERROR: " + str(error.output))
        else:
            print("CalledProcessError")
    except AssertionError as error:
        traceback.print_exc()
        print("ERROR: " + str(error.args))
    except ValueError as error:
        traceback.print_exc()
        print("ERROR: " + str(error.args))
    except Exception as error:
        traceback.print_exc()
        print("ERROR: " + str(error.args))

