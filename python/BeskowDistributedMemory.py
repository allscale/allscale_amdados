# -----------------------------------------------------------------------------
# Author    : Albert Akhriev, albert_akhriev@ie.ibm.com
# Copyright : IBM Research Ireland, 2017-2018
# -----------------------------------------------------------------------------

""" This script runs several simulations with increasing problem size
    utilizing all available CPUs. It saves the execution time of each
    simulation in a file that can be used to plot the scalability profile.
    Essential parameters, listed in the first lines, include the set of
    problem sizes and integration period.
      Each simulation is twofold. First, we run the Python forward solver that
    generates the ground-truth and observations ("ObservationsGenerator.py").
    The Python code itself uses the C++ code running in the special mode for
    generating sensor locations (scenario "sensors"). Second, the C++ data
    assimilation application is launched (scenario "simulation") with
    observations generated by "ObservationsGenerator.py".
      The results of all the simulations are accumulated in the output
    directory and can be visualized later on by the script "Visualize.py".
      The configuration file "amdados.conf" is used in all the simulations with
    modification of three parameters: grid sizes (number of subdomains) in both
    dimensions and integration time. Other parameters remain intact. It is not
    recommended to tweak parameters unless their meaning is absolutely clear.
      If you had modified the parameters, please, consider to rerun this script
    because the results in the output directory a not valid any longer.
      The script was designed to fulfil the formal requirements of the
    Allscale project.
"""
print(__doc__)

from timeit import default_timer as timer
import os, cmd
from RandObservationsGenerator import InitDependentParams, Amdados2D_quick  
from Utility import *
import argparse


# Get subdomain sizes as multiplier factors

## read arguments describing filename, number of nodes and number of threads
##########################
# Arguments and settings #
##########################
parser = argparse.ArgumentParser()
parser.add_argument('filename', type=str, help='filename to write scalability results to')
parser.add_argument('nnodes', type=int, help='number of nodes to execute on')
parser.add_argument('nthreads', type=int, help='number of threads per node')
parser.add_argument('ndomains', type=int, help='number of subdomains')
args = parser.parse_args()

ResultsFileName = args.filename
number_of_nodes = args.nnodes
nthreads = args.nthreads
grid = args.ndomains



# Integration period in seconds.
IntegrationPeriod = 25
IntegrationNsteps = 50

# Configurations for Monitoring and resilience to test
MONITORING = [0]
RESILIENCE = [0]


# Path to the C++ executable.
AMDADOS_EXE = os.path.join(os.getcwd(),"targetcode/amdados_cc")

print('filename = ', ResultsFileName)
print('nnodes = ', number_of_nodes)
print('nthreads = ', nthreads)
print('ndomains = ', grid)


execute_time =  np.zeros([1, 7])

if __name__ == "__main__":
    try:
        # Read configuration file.
        conf = Configuration(os.path.join(os.getcwd(),"amdados.conf"))
        # Create the output directory, if it does not exist.
        conf.output_dir = os.path.join(os.getcwd(),conf.output_dir)
        if not os.path.isdir(conf.output_dir):
            os.mkdir(conf.output_dir)
        # Check existence of "amdados" application executable.
        assert os.path.isfile(AMDADOS_EXE), "amdados executable was not found"
        ## open scalability file and add header
#        HeaderTxt = ["ProblemSize,NNodes, NThreads, ALLSCALE_MONITOR, ALLSCALE_RESILIENCE, TotalRuntime, Throughput(Sdom/s)"]
        time_file = os.path.join(conf.output_dir, ResultsFileName)
        f = open(time_file, 'ab')
        i = 0

        # Modify parameters given the current grid size.
        setattr(conf, "num_subdomains_x", int(grid))
        setattr(conf, "num_subdomains_y", int(grid))
        setattr(conf, "integration_period", int(IntegrationPeriod))
        setattr(conf, "integration_nsteps", int(IntegrationNsteps))
        InitDependentParams(conf)
        conf.PrintParameters()
        config_file = conf.WriteParameterFile(os.path.join(conf.output_dir,"scalability_test.conf"))
        os.sync()
        # Get the starting time.
        start_time = timer()

        # Run C++ data assimilation application.
        for MonitorFlag in MONITORING:
            for ResilienceFlag in RESILIENCE:
                print("##################################################")
                print("Testing Framework for AllScale project")
                print("Simulation by 'amdados' to check scalability and correctness")
                print("Testing Configuration Setup")
                print("GridSize =", grid, "ALLSCALE_MONITOR = ", MonitorFlag,
                "ALLSCALE_RESILIENCE = ", ResilienceFlag)
                print("##################################################")
                print(AMDADOS_EXE, config_file)
                output = subprocess.Popen(["aprun", "-n" +str(number_of_nodes), "-d" + str(nthreads),
                                           AMDADOS_EXE, "--scenario", "benchmark:" +str(grid),"--config", config_file,
                                           "--hpx:threads=" + str(nthreads), "--hpx:bind=none"], stdout=subprocess.PIPE,
                                          env=dict(os.environ, ALLSCALE_MONITOR=str(MonitorFlag), ALLSCALE_RESILIENCE=str(ResilienceFlag)))
                output.wait()
                # Strip the execution time from stdout, both total simulation time
                # and throughput (subdomain/s)
                strip_output = str(output.communicate()[0]).split('\\n')
                for line in strip_output:
                    if re.search("Simulation", line):
                        simtime_string = line
                    if re.search("Throughput", line):
                        throughput_string = line
                simtime_secs = float(simtime_string.split(' ')[2][:-1])
                throughput_secs = float(throughput_string.split(' ')[1])

                # Get the execution time and corresponding (global) problem size
                # and save the current scalability profile into the file.
                problem_size = ( conf.num_subdomains_x * conf.num_subdomains_y )
                execute_time[0, :] = [problem_size, number_of_nodes, nthreads, MonitorFlag, ResilienceFlag, simtime_secs, throughput_secs]
                i += 1
                np.savetxt(f, execute_time)
        f.close()
    except subprocess.CalledProcessError as error:
        traceback.print_exc()
        if error.output is not None:
            print("ERROR: " + str(error.output))
        else:
            print("CalledProcessError")
    except AssertionError as error:
        traceback.print_exc()
        print("ERROR: " + str(error.args))
    except ValueError as error:
        traceback.print_exc()
        print("ERROR: " + str(error.args))
    except Exception as error:
        traceback.print_exc()
        print("ERROR: " + str(error.args))

