\documentclass[]{article}
\usepackage[utf8]{inputenc}
\usepackage{color}   		% may be necessary if you want to color links
\usepackage{hyperref}
\hypersetup {
    colorlinks = true, 		% set true if you want colored links
    linktoc = all,			% set to all if you want both sections and subsections linked
    linkcolor = blue,  		% choose some color if you want links to stand out
}

\title{Notes on ODE and Riccati equation solvers}
\author{Albert Akhriev}
%\date{}
\begin{document}
\maketitle
\tableofcontents

%\begin{abstract}
%\end{abstract}

\section{Introduction}
In this memo we shall briefly review the most popular \textit{explicit} methods for solving ordinary differential equations (ODE). Putting aside numerous variations and implicit extension, for the practical application the ODE integration methods can be roughly classified into four categories: (1) baseline Runge-Kutta algorithm (usually 4th order) with fixed stepsize; (2) widely used Dormand-Prince extension(s)Â to Runge-Kutta method with adaptive stepsize; (3) accurate Bulirsch-Stoer algorithm based on Richardson extrapolation, mostly suitable for the smooth functions; (4) multi-step methods and their particular class called predictor-corrector. More detailed and accurate classification was done by Butcher in \cite{Butcher00}.

\section{ODE Solvers}

\begin{enumerate}
\item A set of $N$ coupled 1st order differential equations for the function vector-function ${\bf y}$ = $(y_1, \ldots, y_N)^T$,
$$
\frac{d {\bf y}(x)}{d x} = {\bf f}(x, {\bf y}).
$$
${\bf f}$ is also called the \textit{flow vector} or \textit{derivative vector}.
\item By convention, a method is called $n$th order if its error term is $O(h^{n+1})$.
\item \textit{Runge-Kutta methods} propagate a solution over an interval by combining the information from several Euler-style steps and then using the information obtained to match a Taylor series expansion up to some higher order, \cite{NR07}. Usually the fastest methods with moderate accuracy.
\item In fact, Runge-Kutta methods constitute a whole family of closely related explicit and implicit algorithms for ODE integration that are best described by means of the Butcher tableau, see \\ \verb|https://en.wikipedia.org/wiki/List_of_Runge-Kutta_methods|
\item \textit{Richardson extrapolation} uses the idea of extrapolating a computed result to the value that would have been obtained if the stepsize had been very much smaller than it actually is, \cite{NR07}. Bulirsch-Stoer method is the 1st practical implementation. Accurate method but can fail in certain cases. Usually Bulirsch-Stoer approach dominates over predictor-corrector one.
\item \textit{Predictor-corrector} or \textit{multistep methods} store the solution along the way, and use those results to extrapolate the solution one step advanced. Extrapolation is then corrected using derivatives. These are best for very smooth functions, \cite{NR07}. The method can be more efficient than Runge-Kutta for many smooth problems where evaluation of ${\bf f}$ is expensive.
\item Authors of \cite{NR07} always recommend an adaptive stepsize control be implemented along with monitoring of internal consistency.
\item High order Runge-Kutta is generally superior to low order methods (such as midpoint), however, \textit{high order does not always mean high accuracy}.
\item Explicit RK methods are not effective for the solution of stiff problems. Because a user often cannot tell in advance whether a problem is stiff, it is useful to be able to diagnose stiffness. Stiffness is a murky concept. In practice what is meant is that a good code intended for stiff problems, typically based on the backward differentiation formulas (BDFs), is much more efficient than a good explicit non-stiff code, typically using RK or Adams formulas. There are many factors that play a role in this and, correspondingly, it is difficult to decide automatically whether a problem is stiff, \cite{Shampine77, Shampine99}.
\end{enumerate}

\subsection{Adaptive stepsize}
\begin{enumerate}
\item A good ODE integrator should exert some adaptive control over its own progress,
making frequent changes in its stepsize. Usually the purpose of this adaptive stepsize
control is to achieve some predetermined accuracy in the solution with minimum
computational effort, \cite{NR07}.
\item Fehlberg \cite{Fehlberg69} proposed a $5$th-order method with six function evaluations where another combination of the six functions gives a $4$th-order method.  By performing one extra calculation, the error in the solution can be estimated and controlled by using the higher-order embedded method that allows for an adaptive stepsize to be determined automatically.
\item The Dormand-Prince method \cite{Dormand80} calculates seven different slopes $k_1$, $k_2$, $k_3$, $k_4$, $k_5$, $k_6$, and $k_7$.	These slopes are then used in two different linear combinations to find two approximations of the next point. One combination provides $O(h^4)$ accuracy while the other\footnote{$y^*_{n+1}$ and $y_{n+1}$ respectively in \cite{NR07}, p.912.} is $O(h^5)$. The coefficients of the $5$th-order approximate were chosen to minimize its error. Dormand-Prince is the algorithm used in the Matlab ODE solver \texttt{ode45}. See also \cite{NR07}, Chapter~17.2.
\item Runge-Kutta approximate solution for Dormand-Prince method (\texttt{ode45}) takes the following form
$$
y_{n+1} = y_n + \sum_{i=1}^6 b_i k_i + O(h^6), \qquad
y^*_{n+1} = y_n + \sum_{i=1}^6 b^*_i k_i + O(h^6),
$$
and the error estimate is
$$
\Delta = y_{n+1} - y^*_{n+1} = \sum_{i=1}^6 (b_i - b^*_i) k_i.
$$
The coefficients $b_i$, $b^*_i$ as well as other constants for Runge-Kutta can be found in \cite{Dormand80, NR07}.
\item Seven function evaluations are needed for Dormand-Prince, but one can be borrowed from the previous step, unless the step was rejected (FSAL: first-same-as-last).
\item Typically, we want to keep bounds
$$
|\Delta| = | y_{n+1} - y^*_{n+1} | \le \mbox{\tt scale} =
\mbox{\tt atol} + \mbox{\tt rtol} \cdot \max{\left(|y_n|,|y_{n+1}|\right)},
$$
where the expression can be understood in terms of vector norm ($L_2$, $L_{\infty}$) or per component, and {\tt atol}, {\tt rtol} stand for \textit{absolute} and \textit{relative} tolerances respectively and can be defined per component as well.
\item We accept the step if \texttt{err} $\le 1$, and reject it otherwise, for example
$$
\mbox{\tt err} = \sqrt{\frac{1}{N}\sum_{i=0}^{N-1}
\left( \frac{\Delta_i}{\mbox{\tt scale}_i} \right)^2}.
$$
\item Suppose we made the step $h_1$ and incurred the error $\mbox{\tt err}_1$. The ``ideal'' step $h_0$ would give us the \textit{desired} integration error $\mbox{\tt err}_0 = 1$. Since $\Delta$ scales as $h^p$, where $p$ is method's order (here $p = 5$), we have
$$
\frac{\mbox{\tt err}_0}{h^p_0} \approx \frac{\mbox{\tt err}_1}{h^p_1}
\quad\longrightarrow\quad
h_0 \approx h_1 \left| \frac{\mbox{\tt err}_0}{\mbox{\tt err}_1} \right|^{1/p}
= h_1 \left| \frac{1}{\mbox{\tt err}_1} \right|^{1/p}
$$
\item If $\mbox{\tt err}_1 > 1$, we decrease the stepsize retrying computation from the current position $x_n$, $y_n$.  If $\mbox{\tt err}_1 < 1$, we can safely increase the stepsize for the next step. \textit{Local extrapolation} means that the $5$th-order value $y_{n+1}$ is used, although the error estimate is applied to the $4$th-order value $y^*_{n+1}$.
\item Recommendations from \cite{NR07}, p.914 how to choose {\tt atol} and {\tt rtol}:
\begin{itemize}
\item If dependent variables differ enormously in magnitude, the reasonable choice is {\tt atol} = 0 and {\tt rtol} = $\varepsilon$, where $\varepsilon = 10^{-6}$ for example.
\item If oscillatory functions that pass through zero and are bounded by some maximum values, then {\tt atol} = {\tt rtol} = $\varepsilon$. This latter choice is the safest in general.
\item Possible way to reduce a global error accumulated over integration period is to set scale proportional to the stepsize, $\mbox{\tt scale}_i$ = $\varepsilon \cdot h \cdot y^{\prime}_i$, for example. However, authors in \cite{NR07} recommend the straightforward ``error per step'' method in most cases. If needed, one can estimate the global error of the solution by integrating again with a reduced tolerance.
\item The most complete and sophisticated way to adapt the stepsize comes from the control theory \cite{Soderlind05, Hairer96, Gustafsson91}. In short, the adaptive stepsize can be selected according to the following simple algorithm
$$
h_{n+1} = S \, h_n \,
\frac{\mbox{\tt err}^{\beta}_{n-1}}{\mbox{\tt err}^{\alpha}_{n}},
\quad \alpha \approx 0.7/p, \quad \beta \approx 0.4/p,
$$
where a safety factor $S$ is slightly smaller than unity, $p$ is the order of integrator (here $p$ = 5), and the coefficients $\alpha$, $\beta$ were chosen as a good compromise between numerical efficiency and stability.
\end{itemize}

\end{enumerate}

\subsection{Test cases}
\begin{enumerate}
%\item Problem:
%$$
%y^{\prime}(t) = y(t) (2-t) t + t - 1, \quad y(0) = 1.
%$$
%Solution:
%$$
%y(t) = \exp{\left(\frac{1}{3}t^2(3-t)\right)}
%\left(
%1 + \int_0^t (\tau - 1) \exp{\left(-\frac{1}{3}\tau^2(3-\tau)\right)} d\tau
%\right).
%$$
\item Problem:
$$
y^{\prime}(t) = (y(t) - 1)^2 (t - 1)^2, \quad y(0) = 0.
$$
Solution:
$$
y(t) = \frac{t^3 - 3 t^2 + 3 t}{t^3 - 3 t^2 + 3 t + 3}.
$$
\item Problem:
$$
y^{\prime}(t) = (3 y + 2 t y) / t^2, \quad t_0 = -1, \quad t_f = -4, \quad y(t_0) = 1.
$$
Solution:
$$
y(t) = t^2 \exp{(-3/t - 3)}.
$$
\item Problem:
$$
y^{\prime}(t) = (y(t))^2 + 1, \quad t_0 = -\pi/2.1, \quad t_f = \pi/2.1, \quad y(t_0) = \tan(t_0).
$$
Solution:
$$
y(t) = tan(t).
$$
The true solution is \textbf{singular} at $t = \pm\pi/2$, so we start a bit closer to zero at $t_0 = -\pi/2.1$ and integrate until $t_f = \pi/2.1$.
\end{enumerate}

\section{Software}

\subsection{Software design}

\begin{enumerate}
\item Two interfaces are defined in the file \texttt{integrator.h}. The first one ---  \texttt{InterfaceDerivative} --- must be inherited by any object capable to compute the first order derivative $y^{\prime}(t)$. The second one --- \texttt{InterfaceIntegrator} --- interfaces any implementation of ODE integrator.
\item Intermediate class \texttt{BaseIntegrator} implements a common functionality of any method. Few functions are left pure-virtual and should be implemented in the class of a concrete method.
\item So far, the following methods are available:
\begin{itemize}
\item Explicit Dormand-Prince 5th order with adaptive step size;
\item Explicit Runge-Kutta 4th order with non-adaptive step size
\end{itemize}
\item Inside the major class --- \texttt{RiccatiSolver} --- there is a new function \texttt{ExplicitDynamic(.)} that uses the new explicit methods. The function implementation is self-explanatory.
\item When method's instance is created by the function \texttt{CreateIntegrator(.)}, whose parameters are explained in the file \texttt{integrator.h}, the default values for tolerances are equal to $\sqrt{\varepsilon}$ and the default value for the minimum step is $\varepsilon$, where $\varepsilon$ is the machine epsilon. The maximum number of steps default to infinity, if not specified.
\item The test suit can be run as: \texttt{make runtest1}. The Python~3 with \texttt{numpy} and \texttt{matplotlib} are required for plotting.
\item For integration with exiting software, one should include all the files in the \texttt{src} directory in the project (only \texttt{integrator.cpp} requires compilation, other files are just headers).
\end{enumerate}

\subsection{Publicly available packages}

\begin{enumerate}
\item \verb|http://www.netlib.org/odepack/|

ODEPACK is a collection of Fortran solvers for the initial value problem for ordinary differential equation systems. The collection is suitable for both stiff and non-stiff systems.

\item \verb|http://www.netlib.org/ode/vode.f|

Variable-coefficient Ordinary Differential Equation solver. There are other solver in the same family, see the {\tt netlib} web-site.

\item \verb|http://www.netlib.org/ode/rksuite/|

RKSUITE is a suite of codes based on Runge-Kutta methods for the numerical solution of the initial value problem for a first order system of ordinary differential equations.  It is the result of decades of research and development of such methods and software by the authors.
\end{enumerate}

\textbf{Warning} in \verb|scipy.integrate.ode| documentation regarding {\tt netlib} code: ``\textit{this integrator is not re-entrant. You cannot have two ode instances using the 'vode' integrator at the same time}''. Possibly just a Python-specific issue.

\subsection{Our custom implementation}

This is something I am going to change. However, the top-level architecture is worth to adopt, see \cite{NR07}, Chapter~17.0.1.
\begin{enumerate}
\item Driver object \textit{Odeint}.
\item Stepper object.
\end{enumerate}

\section{Plan, TODO, open questions, etc}

\subsection{Open questions}
\begin{enumerate}
\item Discuss the existing software: git, launchers, maintenance, test problems.
\item We can start preparing an infrastructure.
\item Design idea: make a pluggable interface -- ODE solver can be easily replaced.
\item How Riccati equation matrices are formed in software? Are they time-dependent?
\item Monitoring stability of the solution to Riccati equation - how?
\item Riccati equation always corresponds to some Kalman filter and vice versa, true?
\item Matrix ${\bf P}$ in Riccati equation, is it a covariance one? Runge-Kutta hardly preserves positive (semi)definiteness of ${\bf P}$, if any. How to monitor this situation?
\item We have to identify an \textit{easy} test problems -- for software validation, and a \textit{hard} problems -- for testing method's stability and accuracy. The test problems should have analytical solutions.
\item What is the integration time? Runge-Kutta may loose accuracy over long period.
\item Runge-Kutta fixed stepsize or adaptive stepsize?
\end{enumerate}

\subsection{Textbook/article recommendations}
\begin{enumerate}
\item Runge-Kutta, even with adaptive stepsize, can be unstable. Also it takes many steps and might be even longer than an implicit method. Recall, matrix-matrix multiplication in $O(N^3)$, the same order is matrix inversion.
\item Bulirsch-Stoer method may not work with singularities.
\item Worth to look at \cite{Benner04, Benner08} -- an efficient implicit method, where the solution is improved via Newton (sub)iterations.
\item In \cite{NR07}. For stiff problems we must use an implicit method if we want to avoid having
tiny stepsizes. (Not all implicit methods are good for stiff problems, but fortunately
some good ones such as the Gear formulas are known.).

Regarding predictor-corrector method. It turns out that for stiff problems functional iteration
will not even converge unless we use tiny stepsizes, no matter how close our prediction is! Thus Newton iteration is usually an essential part of a multistep stiff solver.
\item In \cite{NR07}. The routine StepperSie is an excellent routine for all stiff problems, competitive with the best Gear-type routines. StepperRoss is often better in execution time for moderate size and accuracy (see Webnote [11] in \cite{NR07}).
\end{enumerate} 

\begin{thebibliography}{99}
\bibitem{NR07} William H.~Press, Saul A.~Teukolsky, William T.~Vetterling, and Brian P. Flannery. 2007. Numerical Recipes 3rd Edition: The Art of Scientific Computing (3 ed.). Cambridge University Press, New York, NY, USA.
\bibitem{Shampine77} L.F.~Shampine and H.A.~Watts, The art of writing a Runge-Kutta code, Part I, pp. 257-275 in J.R. Rice, ed., Mathematical Software III, Academic, New York, 1977; and The art of writing a RungeKutta code, II, Appl. Math. Comp. 5 (1979) pp. 93-121.
\bibitem{Shampine99} L.F. Shampine, I. Gladwell, Software based on explicit RK formulas, Applied Numerical Mathematics, Volume 22, Issue 1, 1996, pp. 293-308.
\bibitem{Benner04} P.~Benner and H.~Mena, BDF methods for large-scale differential Riccati equations, in Proc. of Mathematical Theory of Network and Systems, MTNS, 2004.
\bibitem{Benner08} P.~Benner, J.R.~Li and T.~Penzl, Numerical solution of large-scale Lyapunov equations, Riccati equations, and linear-quadratic optimal control problems, Numer. Linear Algebra Appl. 15, pp.755-777, 2008.
\bibitem{Fehlberg69} E.~Fehlberg, Low-order classical Runge-Kutta formulas with step size control and their application to some heat transfer problems, NASA Technical Report 315, 1969.
\bibitem{Dormand80} J.R.~Dormand and P.J.~Prince, ``A family of embedded Runge-Kutta formulae,'' J. Comp. Appl. Math., Vol.~6, 1980, pp.~19-26.
\bibitem{Soderlind05} G.~Soderlind, ``Digital Filters in Adaptive Time-stepping,'' ACM Transactions on Mathematical Software, vol.~29. pp.~1-26, 2003.
\bibitem{Gustafsson91} K.~Gustafsson, ``Control Theoretic Techniques tor Stepsize Selection in Explicit Runge-Kutta Methods.'' ACM Transactions on Mathematical Software, Vol.~17. pp.~533-554, 1991.
\bibitem{Hairer96} E.~Hairer and G.~Wanner, ``Solving Ordinary Differential Equations II. Stiff and Differential-Algebraic Problems,''. 2ed., New York: Springer, 1996.
\bibitem{Butcher00} J.C.~Butcher, ``Numerical methods for ordinary differential equations in the 20th century,'' Journal of Computational and Applied Mathematics, Vol.~125, no.~1-2, pp.~1-29, 2000.
\end{thebibliography}

\end{document}
